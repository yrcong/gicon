<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Learning Similarity between Scene Graphs and Images with Transformers</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Learning Similarity between Scene Graphs and Images with Transformers</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://www.tnt.uni-hannover.de/en/staff/cong/" target="_blank">Yuren Cong</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://www.tnt.uni-hannover.de/en/staff/liao/" target="_blank">Wentong Liao</a><sup>1</sup>,</span>
                  <span class="author-block">
                    <a href="https://www.tnt.uni-hannover.de/en/staff/rosenhahn/" target="_blank">Bodo Rosenhahn</a><sup>1</sup>, </span>
                  <span class="author-block">
                    <a href="https://research.utwente.nl/en/persons/michael-ying-yang" target="_blank">Michael Ying Yang</a><sup>2</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Institute for Information Processing, Leibniz University Hannover</a><sup>1</sup>
                      <br>Scene Understanding Group, University of Twente</a><sup>2</sup></span>
                      <!--<br>Conferance name and year</span>-->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2304.00590" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/yrcong/Learning_Similarity_between_Graphs_Images" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>-->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!--<section class="hero teaser">-->
  <!--<div class="container is-max-desktop">-->
    <!--<div class="hero-body">-->
      <!--<video poster="" id="tree" autoplay controls muted loop height="100%">-->
        <!--&lt;!&ndash; Your video here &ndash;&gt;-->
        <!--<source src="static/videos/banner_video.mp4"-->
        <!--type="video/mp4">-->
      <!--</video>-->
      <!--<h2 class="subtitle has-text-centered">-->
        <!--Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. -->
      <!--</h2>-->
    <!--</div>-->
  <!--</div>-->
<!--</section>-->
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          Scene graph generation is conventionally evaluated by (mean) Recall@K, which measures the ratio of correctly predicted triplets that appear in the ground truth. However, such triplet-oriented metrics cannot capture the global semantic information of scene graphs, and  measure the similarity between images and generated scene graphs. The usability of scene graphs is therefore limited in downstream tasks.
          To address this issue, a framework that can measure the similarity of scene graphs and images is urgently required. Motivated by the successful application of Contrastive Language-Image Pre-training (CLIP), we propose a novel contrastive learning framework consisting of a graph Transformer and an image Transformer to align scene graphs and their corresponding images in the shared latent space.
          To enable the graph Transformer to comprehend the scene graph structure and extract representative features, we introduce a graph serialization technique that transforms a scene graph into a sequence with structural encoding.
          Based on our framework, we introduce R-Precision measuring image retrieval accuracy as a new evaluation metric for scene graph generation and establish new benchmarks for the Visual Genome and Open Images datasets.
          A series of experiments are further conducted to demonstrate the effectiveness of the graph Transformer, which shows great potential as a scene graph encoder.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Method -->
  <section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-two-thirds">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <p>
          we propose a straightforward and robust contrastive learning framework to connect scene graphs and images (GICON) and learn their similarity.
            It consists of a scene graph encoder and an image encoder, both of which are built on the Transformer architecture.
          The input scene graph can be a location-free scene graph (where nodes only represent entity categories) or a location-bound scene graph (where nodes represent entity categories and bounding boxes).
          </p>
          <div class="column">
            <img src="./static/images/framework.png" />
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End method -->

  <div class="hr"></div>

<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
          <div class="columns is-centered has-text-centered">
      <div class="column is-two-thirds">
        <h2 class="title is-3">Similarity between scene graphs and images</h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/vg_qual1.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Results on Visual Genome.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/vg_qual2.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Results on Visual Genome.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/oi_qual1.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Results on Open Images V6.
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/oi_qual2.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Results on Open Images V6.
      </h2>
    </div>
  </div>
</div>
</div>
</div>
</div>
</section>
<!-- End image carousel -->

<section class="section">
  <div class="container is-max-desktop">
    <div class="hr"></div>

    <div class="columns is-centered">

      <!-- Setup -->
      <div class="column">
        <div class="content has-text-justified">
          <h2 class="title is-3">Recall@K (widely-used)</h2>
          <p>
            Recall@K is widely used to evaluate scene graph generation methods.
            It calculates the fraction of ground truth triplets that appear in the top K confident triplet predictions.
            Due to the long tail issue in the scene graph dataset such as Visual Genome, mean Recall@K is proposed.
            However, both Recall@K and mean Recall@K have limitations, as they critically compare the predicted triplet set with the ground truth triplet set and are thus sensitive to noise and bias in the dataset annotations
           </p>
            <img src="static/images/teaser.png" alt="MY ALT TEXT"/>
        </div>
      </div>

      <!-- Views-->
      <div class="column">
        <div class="content has-text-justified">
        <h2 class="title is-3">R-Precision (new!)</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
            Therefore, we propose R-Precision based on GICON for scene graph generation evaluation.
            R-Precision measures the retrieval accuracy when retrieving the matching image from K image candidates using the generated scene graph as a query.
            We use the scene graph and image representations provided by GICON to compute the similarity score for retrieval.
            Compared to triplet-oriented metrics, R-Precision based on GICON is more robust to the perturbation of single triplets.
            </p>
            <img src="static/images/rcurve.png" alt="MY ALT TEXT"/>
          </div>
          </div>

        </div>

      </div>
    </div>



    <div class="columns is-centered has-text-centered">
      <div class="column is-fourth-fifths">
        <h2 class="title is-3">New Benchmark of Scene Graph Generation Models</h2>
        <div class="content has-text-justified">
          <p>
          We benchmark different scene graph generation models on the Visual Genome dataset.
            6 two-stage methods and 4 one-stage methods are re-evaluated using R-Precision (K=10/50/100) for location-free scene graphs (LF Graph) and location-bound scene graphs (LB Graph).
          </p>
          <div class="column">
            <img src="./static/images/table.PNG" />
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{cong2023learning,
  title={Learning Similarity between Scene Graphs and Images with Transformers},
  author={Cong, Yuren and Liao, Wentong and Rosenhahn, Bodo and Yang, Michael Ying},
  journal={arXiv preprint arXiv:2304.00590},
  year={2023}
}
</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
